. Product narrative (so the agent “gets” the app)
We are creating a stand‑alone practice‑test platform for students preparing to sit CPC‑500‑series insurance exams.

A signed‑in learner chooses a course, then one of three 85‑question practice tests.

Each time a test starts, every question is served exactly once, in random order.

A right answer moves the learner forward; a wrong answer “flips” the card and reveals a chatbot powered by OpenRouter that can cite the course material and answer follow‑up questions.

A collapsible left rail shows every question number, colour‑coded green/red for correct/incorrect and always clickable for review.

Progress (per‑question, per‑test, per‑course) is saved to the backend so learners can leave and resume without losing their place.

2. High‑level architecture
Frontend – Single‑page app with three principal views:

Dashboard (lists enrolled courses ► practice tests ► “Start Test”).

Test Player (question card, progress bar, left rail, chatbot flip‑side).

Admin (CRUD for courses, tests, questions, GPT prompt settings).

Backend – REST (or GraphQL, agent’s choice) with JWT‑based auth, a relational DB, and an LLM service wrapper that forwards prompts to OpenRouter.

Data seed – Import course/question JSON at start‑up. A sample payload is provided below.

3. Data model the agent should implement
Describe entities relationally, no SQL schema needed here—agent can decide types:

User: id, name, email, hashedPassword, createdAt.

Course: id, title, description.

PracticeTest: id, courseId → Course, title (e.g. “Practice Test A”), questionCount (85).

Question: id, courseId, originalQuestionNumber, LOID.

QuestionVersion: id, questionId → Question, versionNumber, topicFocus, questionText, answerChoices[] (array of strings), correctAnswer (single letter).

UserTestRun: id, userId → User, practiceTestId → PracticeTest, startedAt, completedAt.

UserAnswer: id, userTestRunId → UserTestRun, questionVersionId → QuestionVersion, chosenAnswer, isCorrect, answeredAt.

4. Question‑selection and randomisation logic
When the learner clicks “Start Test” create a UserTestRun.

Fetch all 85 originalQuestionNumbers for that test’s course.

For each original number, randomly pick one of its three versions.

Shuffle the resulting 85‑item array and persist it as the “draw order” (so refreshes do not reshuffle).

Serve questions in sequence; allow back/forward navigation via the left rail.

Edge‑case rule: if the learner abandons, reopening the test resumes at the first unanswered question.

5. Card‑flip feedback and chatbot contract
After submission:

If correct – display a green confirmation, enable Next.

If incorrect – animate a 180° Y‑axis flip of the card, revealing:

The correct answer.

A chat window initialised with a system prompt containing:

pgsql
Copy
You are a course‑assistant AI.  The learner chose answer “X”; the correct answer is “Y”.  
Here is the full question stem and answer set, plus any course reference text that explains the concept.  
Explain why the correct answer is correct, why the chosen answer is not, and invite follow‑up questions.  
Keep replies under 150 words unless the learner requests more depth.
The frontend sends one JSON message to the backend: {questionVersionId, chosenAnswer, correctAnswer}; the backend packages that with reference material and relays it to OpenRouter. Return the LLM’s streaming response to the client.

6. Dashboard & learner progress
When the learner lands on /dashboard the backend must supply:

For every course – percentage of total questions ever answered correctly.

For every test in that course – status Not started / In progress / Completed plus current score if finished.

Display these numbers as simple chips beneath each card; avoid heavy charts for MVP.

7. Admin panel essentials
CRUD screens for Course, PracticeTest, Question, QuestionVersion.

Bulk JSON import/export for questions (upload a file, or paste in a text box).

Editable GPT settings:

OpenRouter model name (default empty uses tenant default).

System prompt override.

Temperature, max tokens, top‑p sliders.

Simple API key vault so keys aren’t hard‑coded.

Only users flagged isAdmin see this panel.

8. Authentication & authorisation
Email + password registration; email verification not required for now.

Session kept in localStorage via JWT.

Route guards:

/admin/** → admin only.

/test/:runId → owner of the run or admin.

9. Acceptance checklist (handy for the agent’s automated tests)
A learner can register, log in, pick a course, start a test, answer questions, flip cards on wrong answers, quit, return, finish, and see updated dashboard stats—all without manual DB tweaks.

Each of the 85 questions appears once and only once per run.

Refreshing the browser never loses answered‑question state.

Admin can import the sample JSON below and immediately assign it to a course/test.

The chatbot correctly receives the context payload and responds (mock if OpenRouter key missing).

10. Sample question‑bank payload
Here is the truncated set the product owner provided. Use it to prove your import routine works; the full bank will be the same shape.

json
Copy
[
  {
    "originalQuestionNumber": 1,
    "LOID": "11597",
    "versions": [
      {
        "versionNumber": 1,
        "topicFocus": "How insurance facilitates access to auto ownership by transferring risk",
        "questionText": "...",
        "answerChoices": ["A. ...", "B. ...", "C. ...", "D. ..."],
        "correctAnswer": "D"
      },
      { "versionNumber": 2, "...": "..." },
      { "versionNumber": 3, "...": "..." }
    ]
  },

  { "originalQuestionNumber": 2, "...": "..." },

  { "originalQuestionNumber": 3, "...": "..." }

  /* continues up to 15 here; real bank will contain 85 originals × 3 versions */
]
The parser should create one Question row per unique originalQuestionNumber, and three QuestionVersion rows linked to it.

Final note to the Replit AI agent
Feel free to scaffold with your favourite full‑stack template (e.g., FastAPI + React/Vite, Prisma ORM, etc.).

Write seed scripts for the sample payload.

Prioritise test coverage of the randomisation engine and state restoration.

Ship an .env.sample with placeholders for DB URL and OpenRouter key.

Once all acceptance items pass locally, push to the Replit workspace and hand back a URL plus repo link.