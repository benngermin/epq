Admin Interface Expansion — Question Editing & Static Explanation Generation
(Agent-Ready Requirements • Please do not add scope beyond what’s here)

Overview
We are replacing the brittle “Refresh All / Refresh Single Question Set” pipeline with a direct, in-app admin workflow for viewing, editing, creating, deleting (archiving), reordering, and explaining questions. After reviewing this spec, collaborate with me on an in-depth implementation plan that covers the full scope below.

A. Source of Truth & Deprecations
1) Single source of truth: This app’s database (DB).  
2) Deprecate external sync: We will fully deprecate all syncing from the external content repository for question sets/questions. After launch, I will remove question-set functionality and storage from the third-party content repository.  
3) Preserve external course IDs: Continue storing and using all external course IDs (unchanged). Only the editing/maintenance of question sets and questions moves into this app.

B. Admin Navigation & UX
Target flow: Admin opens a Course → opens a Question Set → sees all Questions in that set → selects/edits → saves.
1) Course → Set → Questions view:
   - Display all questions in a “digestible” list optimized for rapid edits (not necessarily a table).
   - Inline editing for the most commonly edited fields: question text and correct answer(s).
   - Other type-specific fields must also be editable without forcing users into nested/embedded screens.
   - Must comfortably handle ~30–100 questions per set (2–3 sets per course), with standard scroll.
2) Save & confirmations:
   - On Save: one confirmation modal per question save (after all edits to that question). No per-field modals.
   - On Delete (Archive): one confirmation modal.
3) Unsaved changes warning:
   - Warn on navigation with unsaved changes if implementation complexity is moderate or lower; otherwise skip.

C. Create / Edit / Delete (Archive) Semantics
1) Create:
   - Require selecting the question_type before creation; show the appropriate fields for that type.
   - Require selecting explanation mode before creation: AI or Static (see Section E/F).
2) Edit:
   - All fields of a question must be editable within the question set view as described in B(1).
3) Delete → Archive (soft delete):
   - Deleting a question archives it (it no longer appears in the main list).
   - Provide an “Archived” tab per question set where admins can “Recover” (unarchive) questions.

D. Ordering & Remix
1) Drag-to-reorder within a question set.  
2) “Remix” button: randomizes the order of all questions in the set.  
3) Default order is the current order until changed.

E. Explanation Modes (AI vs Static) & Persistence Rules
Terminology:
- AI explanation = When a learner answers a question in the front-end, we make a real-time API call to generate feedback/explanation.
- Static explanation = A pre-written/pre-generated explanation stored in our DB and shown immediately to learners.

Data fields (existing):
- is_static_answer (boolean)
- static explanation (string)

Defaults & switching:
1) Default: AI explanation (is_static_answer = false).  
2) Switch AI → Static: nothing special to preserve.  
3) Switch Static → AI: show a confirm modal; if confirmed, delete the stored static explanation and set is_static_answer = false.

F. Static Explanation Generation (OpenRouter) — Exact Flow
1) Trigger: Admin clicks “Generate Static Explanation” for a question that is set to use static explanations (or is being switched to static).
2) Pre-step: Retrieve learning content from the content repository by the question’s LOID (we already have app logic for this API call—reuse it).  
3) Compose prompt: Use the exact prompt template below, injecting the question fields and the retrieved learning content into the variables as indicated.  
4) OpenRouter call:
   - Use the in-app configurable model and system message (see Section G).
   - Hard-code max_tokens = 32000.
   - Set reasoning = "medium".
   - Do NOT stream output; show a simple loading indicator until the response returns.
5) Review + Save:
   - Display the generated text to the admin.
   - Do NOT persist the generated text until the admin clicks Save for that question.
   - If the admin navigates away or cancels without saving, the last saved static explanation (if any) remains unchanged.

Prompt Template (verbatim; use as-is and replace variables exactly)
--------------------------------------------------------------------------------
Output the below structure within the <output> exactly. **DO NOT include the <output> tags themselves**:

<output>

Correct Answer: [Insert only the correct answer text exactly as is. 
- For blanks, return only the answer itself (e.g., "agreed value"), not the label "First Blank" or "Second Blank".
- For multiple-choice, return only the letter (e.g., "A").
- For multiple-response, return letters in alphabetical order (e.g., "A,C,D").]

Explanation: Provide a concise explanation of why the above answer is correct, based only on <learning_content>.

- Use  internally to identify the relevant policy type or concept, but do NOT repeat or display the question text in the output.
- Use only the subsection that directly matches the concept (e.g., PAP/PIP vs. HO, liability vs. property). 
- Ignore examples, scenarios, and "Check Your Understanding" sections. 
- Do not display subsection names explicitly in the output. Simply use them to guide the explanation.
- Focus only on why the correct answer is correct. Maintain a concise, neutral, instructional tone.

If the question involves math:
- Always format the solution in four clear steps:
 - **Step 1 (Formula):** Show the general formula symbolically, with no numbers.
 - **Step 2 (Substitute Values):** Rewrite the formula, plugging in the actual values.
 - **Step 3 (Solve):** Perform the calculation, showing intermediate results if needed.
 - **Step 4 (Final Answer):** State the final result clearly, with units or context in plain language.
- Do not combine steps; keep each step on its own line.
- Follow rounding instructions in the question; if none, round percentages to two decimals and dollar values to the nearest whole number.

If the question is conceptual:
- Summarize the principle in 3–6 lines.
- Keep neutral and instructional; do not use first person.

Source:
- At the end, display the most relevant URL from <learning_content> as HTML: <a href="[URL]">[Title]</a>.
- If no URL exists in the chosen subsection, use the first URL in <learning_content>.

<output>

----
<correct_answer>

</correct_answer>
---
<learning_content>

</learning_content>
-------------------------------------------------------------------------------

Note: Before calling OpenRouter, always fetch <learning_content> from the content repository using the question’s LOID. Reuse existing app logic for this API call.

G. OpenRouter Settings (Configurable)
1) Configurable in-app: model, system message.  
2) Use the same OpenRouter model list already available in our other prompt admin panel.  
3) Hard-code: max_tokens = 32000; reasoning = "medium".  
4) All admins can edit these settings.  
5) Settings are global.

H. Validation & Error Handling
1) Validation: No additional validation beyond what is implied by the JSON schema and question type selection (e.g., show multiple-choice fields when type = multiple_choice, etc.). Assume admins will edit correctly.  
2) Errors: Show toast notifications only for failures in:
   - Learning content fetch (content repository API), and
   - OpenRouter generation API.
   No specialized rate limiting or retry/backoff is required.

I. Current System to Replace
Remove reliance on:
1) “Refresh All / Refresh Single Question Set” → content repository fetch of a large JSON array → parse into individual questions → complex matching/versioning/order logic.  
2) Airtable pipeline for static explanations (all existing static explanations are already uploaded into this app).

J. Migration / Existing Data
1) All existing static explanations are already in this app.  
2) Any question that currently has a static explanation should appear as “Static” (is_static_answer = true) in the new UI and behave accordingly.

K. Canonical Question JSON Example (for field names & types)
Use this structure for editing/creation UIs and persistence expectations.

{
  "question_set_name": "Course Name Here",
  "bubble_course_unique_id": "unique_id_here",
  "bubble_question_set_unique_id": "question_set_unique_id_here",
  "question_set_number": 1,
  "questions": [
    {
      "question_number": 1,
      "loid": "00000",
      "question_type": "multiple_choice",
      "question_text": "Question text here",
      "answer_choices": [
        "A. Choice 1",
        "B. Choice 2", 
        "C. Choice 3",
        "D. Choice 4"
      ],
      "correct_answer": "A"
    },
    {
      "question_number": 2,
      "loid": "00001",
      "question_type": "numerical_entry",
      "question_text": "Numerical question here",
      "answer_choices": [],
      "correct_answer": "100",
      "acceptable_answers": ["100", "100.0", "100.00"]
    },
    {
      "question_number": 3,
      "loid": "00002",
      "question_type": "select_from_list",
      "question_text": "Question with _____ blank here",
      "blanks": [
        {
          "blank_id": 1,
          "answer_choices": ["choice 1", "choice 2", "choice 3", "choice 4"],
          "correct_answer": "choice 1"
        }
      ]
    },
    {
      "question_number": 4,
      "loid": "00003",
      "question_type": "drag_and_drop",
      "question_text": "Drag and drop question here",
      "answer_choices": ["Item 1", "Item 2", "Item 3", "Item 4"],
      "drop_zones": [
        {
          "zone_id": 1,
          "zone_label": "Zone 1 Label"
        },
        {
          "zone_id": 2,
          "zone_label": "Zone 2 Label"
        }
      ],
      "correct_answer": {
        "zone_1": ["Item 1", "Item 2"],
        "zone_2": ["Item 3", "Item 4"]
      }
    },
    {
      "question_number": 5,
      "loid": "00004",
      "question_type": "either_or",
      "question_text": "Either/or question here",
      "answer_choices": [
        "A) Option A",
        "B) Option B"
      ],
      "correct_answer": "A) Option A"
    },
    {
      "question_number": 6,
      "loid": "00005",
      "question_type": "multiple_response",
      "question_text": "Multiple response question. Select all that apply:",
      "answer_choices": ["Choice 1", "Choice 2", "Choice 3", "Choice 4", "Choice 5"],
      "correct_answer": ["Choice 1", "Choice 2", "Choice 3"],
      "allow_multiple": true
    },
    {
      "question_number": 7,
      "loid": "00006",
      "question_type": "short_answer",
      "question_text": "Fill-in-the-blank: A _____ is something and a _____ is something else.",
      "answer_choices": [],
      "correct_answer": "answer 1, answer 2",
      "acceptable_answers": ["answer 1, answer 2", "alternate answer"],
      "case_sensitive": false
    }
  ]
}

L. Out of Scope (for this change set)
- Bulk editing across many questions at once.
- Additional validation beyond the schema/type-driven UI constraints.
- Rate limiting/retry/backoff policies.
- Streaming LLM output.
- Removing legacy screens (I will handle later).

Next Step
Use the above as the definitive requirements. Propose an in-depth implementation plan that: (1) covers CRUD, ordering/remix, archiving/recover, explanation mode switching, and the generation flow; (2) specifies UI states and DB updates for each action; and (3) details the OpenRouter + learning content retrieval sequence exactly as described.
