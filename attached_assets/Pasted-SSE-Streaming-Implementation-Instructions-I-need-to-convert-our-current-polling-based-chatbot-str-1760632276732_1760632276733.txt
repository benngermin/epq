SSE Streaming Implementation Instructions

  I need to convert our current polling-based chatbot streaming to true
  Server-Sent Events (SSE) streaming. I want to use SSE instead of WebSockets
  because our data only flows serverâ†’client, SSE is simpler, and it's already
  what OpenRouter sends us - we'll just pass it through directly instead of
  buffering and polling.

  Phase 1: Backend Implementation

  Step 1: Create Helper Function to Build System Message

  In server/routes.ts, I need to extract the system message building logic that's
   currently duplicated in processStreamInBackground() into a reusable function:

  function buildSystemMessage(
    questionVersion: any,
    chosenAnswer: string,
    courseMaterial: any,
    activePrompt: any
  ): string {
    const formattedChoices = questionVersion.answerChoices.join('\n');
    const selectedAnswer = chosenAnswer?.trim() || "No answer was selected";
    const sourceMaterial = courseMaterial?.content
      || questionVersion.topicFocus
      || "No additional source material provided.";
    const effectiveCorrectAnswer =
  extractCorrectAnswerFromBlanks(questionVersion);

    const systemPromptTemplate = activePrompt?.promptText || `[default prompt 
  template from existing code]`;

    return systemPromptTemplate
      .replace(/\{\{QUESTION_TEXT\}\}/g, questionVersion.questionText)
      .replace(/\{\{ANSWER_CHOICES\}\}/g, formattedChoices)
      .replace(/\{\{SELECTED_ANSWER\}\}/g, selectedAnswer)
      .replace(/\{\{CORRECT_ANSWER\}\}/g, effectiveCorrectAnswer)
      .replace(/\{\{COURSE_MATERIAL\}\}/g, sourceMaterial);
  }

  Step 2: Create Direct SSE Streaming Function

  In server/routes.ts, add this new function that streams directly from
  OpenRouter to the client without buffering:

  async function streamOpenRouterDirectly(
    res: Response,
    messages: Array<{role: string, content: string}>,
    conversationHistory: Array<{role: string, content: string}>,
    userId?: number
  ) {
    const apiKey = process.env.OPENROUTER_API_KEY;

    if (!apiKey) {
      res.write('data: {"type":"error","message":"OpenRouter API key not 
  configured"}\n\n');
      res.end();
      return;
    }

    try {
      const response = await
  fetch("https://openrouter.ai/api/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${apiKey}`,
          "Content-Type": "application/json",
          "HTTP-Referer": process.env.APP_URL || "http://localhost:5000",
        },
        body: JSON.stringify({
          model: "anthropic/claude-sonnet-4",
          messages,
          temperature: 0,
          max_tokens: 56000,
          stream: true,
          reasoning: { effort: "medium" }
        }),
      });

      if (!response.ok) {
        res.write(`data: {"type":"error","message":"OpenRouter API error"}\n\n`);
        res.end();
        return;
      }

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      let fullResponse = "";
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        buffer += chunk;

        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (!line.trim() || !line.startsWith('data: ')) continue;

          const data = line.slice(6).trim();

          if (data === '[DONE]') {
            const updatedHistory = [
              ...conversationHistory,
              messages[messages.length - 1],
              { role: "assistant", content: fullResponse }
            ];

            res.write(`data: ${JSON.stringify({
              type: "done",
              conversationHistory: updatedHistory
            })}\n\n`);
            res.end();
            return;
          }

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices?.[0]?.delta?.content;

            if (content) {
              fullResponse += content;
              res.write(`data: ${JSON.stringify({
                type: "chunk",
                content: content
              })}\n\n`);
            }

            const finishReason = parsed.choices?.[0]?.finish_reason;
            if (finishReason) {
              const updatedHistory = [
                ...conversationHistory,
                messages[messages.length - 1],
                { role: "assistant", content: fullResponse }
              ];

              res.write(`data: ${JSON.stringify({
                type: "done",
                conversationHistory: updatedHistory
              })}\n\n`);
              res.end();
              return;
            }
          } catch (e) {
            // Skip unparseable chunks
          }
        }
      }

      // Fallback completion
      const updatedHistory = [
        ...conversationHistory,
        messages[messages.length - 1],
        { role: "assistant", content: fullResponse }
      ];
      res.write(`data: ${JSON.stringify({type: "done", conversationHistory: 
  updatedHistory})}\n\n`);
      res.end();

    } catch (error) {
      res.write(`data: {"type":"error","message":"Streaming failed"}\n\n`);
      res.end();
    }
  }

  Step 3: Create New SSE Endpoint

  In server/routes.ts, add this new endpoint (place it near the existing
  /api/chatbot/stream-init endpoint):

  app.post("/api/chatbot/stream-sse", requireAuth, aiRateLimiter.middleware(),
  async (req, res) => {
    try {
      const { questionVersionId, chosenAnswer, userMessage, isMobile,
  conversationHistory } = req.body;
      const userId = req.user!.id;

      // Set SSE headers
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache, no-transform');
      res.setHeader('Connection', 'keep-alive');
      res.setHeader('X-Accel-Buffering', 'no');

      res.write('data: {"type":"connected"}\n\n');

      // Get question and context (same logic as processStreamInBackground)
      const questionVersion = await
  storage.getQuestionVersion(questionVersionId);
      if (!questionVersion) {
        res.write('data: {"type":"error","message":"Question not found"}\n\n');
        res.end();
        return;
      }

      const baseQuestion = await storage.getQuestion(questionVersion.questionId);
      const courseMaterial = baseQuestion?.loid
        ? await storage.getCourseMaterialByLoid(baseQuestion.loid)
        : null;

      const aiSettings = await storage.getAiSettings();
      const activePrompt = await storage.getActivePromptVersion();

      // Build system message
      const systemMessage = buildSystemMessage(questionVersion, chosenAnswer,
  courseMaterial, activePrompt);

      // Prepare messages array
      let messages = [];
      if (conversationHistory && conversationHistory.length > 0) {
        messages = [...conversationHistory];
        messages.push({
          role: "user",
          content: userMessage || "Please provide feedback on my answer."
        });
      } else {
        messages.push({ role: "system", content: systemMessage });
        messages.push({ role: "user", content: "Please provide feedback on my 
  answer." });
      }

      // Stream directly from OpenRouter
      await streamOpenRouterDirectly(res, messages, conversationHistory || [],
  userId);

    } catch (error) {
      console.error("SSE streaming error:", error);
      res.write(`data: {"type":"error","message":"${error.message}"}\n\n`);
      res.end();
    }
  });

  Phase 2: Frontend Implementation

  Step 4: Create SSE Hook

  Create new file client/src/hooks/use-sse-stream.ts:

  import { useState, useRef } from 'react';

  interface SSEMessage {
    type: 'connected' | 'chunk' | 'done' | 'error';
    content?: string;
    conversationHistory?: Array<{ role: string; content: string }>;
    message?: string;
  }

  interface UseSSEStreamOptions {
    onChunk?: (content: string) => void;
    onComplete?: (conversationHistory: any[]) => void;
    onError?: (error: string) => void;
  }

  export function useSSEStream(options: UseSSEStreamOptions = {}) {
    const [isStreaming, setIsStreaming] = useState(false);
    const abortControllerRef = useRef<AbortController | null>(null);

    const startStream = async (
      questionVersionId: number,
      chosenAnswer: string,
      userMessage?: string,
      conversationHistory?: any[]
    ) => {
      stopStream();
      setIsStreaming(true);

      try {
        abortControllerRef.current = new AbortController();

        const isDemo = window.location.pathname.startsWith('/demo');
        const endpoint = isDemo ? '/api/demo/chatbot/stream-sse' :
  '/api/chatbot/stream-sse';
        const isMobile = window.innerWidth < 768;

        const response = await fetch(endpoint, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          credentials: 'include',
          body: JSON.stringify({
            questionVersionId,
            chosenAnswer,
            userMessage,
            isMobile,
            conversationHistory: conversationHistory || []
          }),
          signal: abortControllerRef.current.signal
        });

        if (!response.ok) throw new Error(`HTTP error! status: 
  ${response.status}`);

        const reader = response.body?.getReader();
        const decoder = new TextDecoder();
        if (!reader) throw new Error('No response stream available');

        let buffer = '';

        while (true) {
          const { done, value } = await reader.read();
          if (done) {
            setIsStreaming(false);
            break;
          }

          const chunk = decoder.decode(value, { stream: true });
          buffer += chunk;

          const lines = buffer.split('\n');
          buffer = lines.pop() || '';

          for (const line of lines) {
            if (!line.trim() || !line.startsWith('data: ')) continue;

            const data = line.slice(6).trim();

            try {
              const message: SSEMessage = JSON.parse(data);

              if (message.type === 'chunk' && message.content) {
                options.onChunk?.(message.content);
              } else if (message.type === 'done') {
                setIsStreaming(false);
                options.onComplete?.(message.conversationHistory || []);
              } else if (message.type === 'error') {
                setIsStreaming(false);
                options.onError?.(message.message || 'Stream error');
              }
            } catch (e) {
              console.warn('Failed to parse SSE message:', data);
            }
          }
        }

      } catch (error: any) {
        if (error.name !== 'AbortError') {
          console.error('SSE stream error:', error);
          setIsStreaming(false);
          options.onError?.(error.message || 'Failed to connect to stream');
        }
      }
    };

    const stopStream = () => {
      abortControllerRef.current?.abort();
      abortControllerRef.current = null;
      setIsStreaming(false);
    };

    return { isStreaming, startStream, stopStream };
  }

  Step 5: Update ChatInterface Component

  In client/src/components/chat-interface.tsx, I need to:

  1. Import the new hook
  2. Replace the entire streamChatResponse function and polling logic with SSE
  3. Keep all UI/UX the same

  Replace the streaming logic:

  import { useSSEStream } from '@/hooks/use-sse-stream';

  export function ChatInterface({ questionVersionId, chosenAnswer, correctAnswer 
  }: ChatInterfaceProps) {
    // Keep existing state
    const [messages, setMessages] = useState<ChatMessage[]>([]);
    const [conversationHistory, setConversationHistory] =
  useState<ConversationMessage[]>([]);
    const [userInput, setUserInput] = useState("");
    const [hasRequestedInitial, setHasRequestedInitial] = useState(false);
    const { toast } = useToast();
    const messagesEndRef = useRef<HTMLDivElement>(null);
    const currentMessageIdRef = useRef<string>("");

    const currentQuestionKey =
  `${questionVersionId}-${chosenAnswer}-${correctAnswer}`;

    // Replace polling with SSE hook
    const { isStreaming, startStream, stopStream } = useSSEStream({
      onChunk: (content) => {
        setMessages(prev => prev.map(msg =>
          msg.id === currentMessageIdRef.current && msg.isStreaming
            ? { ...msg, content: msg.content + content }
            : msg
        ));
        setTimeout(() => messagesEndRef.current?.scrollIntoView({ behavior:
  "smooth" }), 10);
      },
      onComplete: (newConversationHistory) => {
        setMessages(prev => prev.map(msg =>
          msg.id === currentMessageIdRef.current && msg.isStreaming
            ? { ...msg, isStreaming: false }
            : msg
        ));
        if (newConversationHistory) {
          setConversationHistory(newConversationHistory);
        }
        currentMessageIdRef.current = "";
      },
      onError: (error) => {
        setMessages(prev => prev.filter(msg => msg.id !==
  currentMessageIdRef.current));
        currentMessageIdRef.current = "";
        toast({
          title: "Error",
          description: "Failed to get response from AI assistant",
          variant: "destructive",
        });
      }
    });

    // Replace old streamChatResponse with new SSE version
    const streamChatResponse = async (userMessage?: string) => {
      if (isStreaming) return;

      const messageId = Date.now().toString() + '_' +
  Math.random().toString(36).substring(2, 9);
      currentMessageIdRef.current = messageId;

      setMessages(prev => [{
        role: "assistant",
        content: "",
        isStreaming: true,
        id: messageId
      }, ...prev]);

      await startStream(questionVersionId, chosenAnswer, userMessage,
  conversationHistory);
    };

    // Keep all existing useEffect hooks and UI rendering exactly the same
    // Just update the cleanup in the reset effect:
    useEffect(() => {
      stopStream();
      setHasRequestedInitial(false);
      setMessages([]);
      setConversationHistory([]);
    }, [currentQuestionKey]);

    // Rest of component unchanged (handleSendMessage, UI, etc.)
  }

  Remove these old refs and state that are no longer needed:
  - streamingContent and setStreamingContent
  - forceRender and setForceRender
  - streamingDisplayRef
  - isStreamingRef
  - streamingContentRef
  - currentStreamIdRef
  - streamingMessageIdRef
  - abortControllerRef

  Remove the entire old polling logic (the while loop with
  fetch(/api/chatbot/stream-chunk/...)).

  Phase 3: Testing

  I need to test:

  1. Initial explanation - Answer a question wrong, verify chatbot appears and
  streams
  2. Follow-up questions - Ask follow-up, verify conversation history works
  3. Question changes - Move to different question, verify old stream aborts
  4. Mobile - Test on mobile browser, verify isMobile flag works
  5. Demo mode - Test /demo/... routes use demo endpoint
  6. Errors - Disconnect network mid-stream, verify error handling
  7. Rapid follow-ups - Send multiple questions quickly, verify no conflicts

  Phase 4: Deployment

  I'll deploy in stages:

  1. Deploy backend only - New SSE endpoint available but not used yet
  2. Test with curl - Verify SSE endpoint works: curl -N -H "Authorization: 
  Bearer TOKEN" -H "Content-Type: application/json" -d
  '{"questionVersionId":1,"chosenAnswer":"test"}'
  https://app.com/api/chatbot/stream-sse
  3. Deploy frontend - ChatInterface now uses SSE
  4. Monitor for 24 hours - Watch error rates, verify no issues
  5. Clean up old code - After stable, remove polling endpoints and activeStreams
   Map

  Important Notes

  - Keep the existing polling endpoints for now as backup
  - Don't change any UI/UX - only the underlying transport mechanism
  - The SSE hook accumulates chunks client-side (just like old polling did)
  - Conversation history format stays exactly the same
  - Mobile detection and demo mode detection work identically

  That's the complete implementation. Start with Phase 1 backend changes, then
  Phase 2 frontend, then test thoroughly before deploying.